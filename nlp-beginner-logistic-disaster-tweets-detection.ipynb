{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading needed packages","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nimport nltk    \nimport matplotlib.pyplot as plt    \n\nfrom nltk.corpus import twitter_samples\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-21T06:42:59.136248Z","iopub.execute_input":"2023-07-21T06:42:59.137828Z","iopub.status.idle":"2023-07-21T06:42:59.145825Z","shell.execute_reply.started":"2023-07-21T06:42:59.137781Z","shell.execute_reply":"2023-07-21T06:42:59.144306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load some useful functions below","metadata":{}},{"cell_type":"markdown","source":"Use below function to process tweet, it would return a list of words","metadata":{}},{"cell_type":"code","source":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n  \n    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n\n\n    tweet = re.sub(r'#', '', tweet)\n\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-21T06:42:59.148478Z","iopub.execute_input":"2023-07-21T06:42:59.148856Z","iopub.status.idle":"2023-07-21T06:42:59.163924Z","shell.execute_reply.started":"2023-07-21T06:42:59.148826Z","shell.execute_reply":"2023-07-21T06:42:59.163032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use below function to build the frequency matrix, in the form of ('word',1/0):number of words","metadata":{}},{"cell_type":"code","source":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.165183Z","iopub.execute_input":"2023-07-21T06:42:59.166502Z","iopub.status.idle":"2023-07-21T06:42:59.177857Z","shell.execute_reply.started":"2023-07-21T06:42:59.166461Z","shell.execute_reply":"2023-07-21T06:42:59.176422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore datasets","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv').sample(2000)\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain = train.append(test).drop_duplicates(keep=False)\n\nvalidation = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.179489Z","iopub.execute_input":"2023-07-21T06:42:59.179797Z","iopub.status.idle":"2023-07-21T06:42:59.273378Z","shell.execute_reply.started":"2023-07-21T06:42:59.17977Z","shell.execute_reply":"2023-07-21T06:42:59.272476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download stop words list","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.276087Z","iopub.execute_input":"2023-07-21T06:42:59.276736Z","iopub.status.idle":"2023-07-21T06:42:59.28572Z","shell.execute_reply.started":"2023-07-21T06:42:59.276701Z","shell.execute_reply":"2023-07-21T06:42:59.284455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"train_df1 = list(train[train['target']==1]['text'])\ntrain_df0 = list(train[train['target']==0]['text'])\nprint(len(train_df1))\nprint(len(train_df0))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.287142Z","iopub.execute_input":"2023-07-21T06:42:59.28752Z","iopub.status.idle":"2023-07-21T06:42:59.300993Z","shell.execute_reply.started":"2023-07-21T06:42:59.287489Z","shell.execute_reply":"2023-07-21T06:42:59.299982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = train_df1 + train_df0\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nlabels = np.append(np.ones((len(train_df1))),np.zeros((len(train_df0))))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.302391Z","iopub.execute_input":"2023-07-21T06:42:59.3029Z","iopub.status.idle":"2023-07-21T06:42:59.311119Z","shell.execute_reply.started":"2023-07-21T06:42:59.302868Z","shell.execute_reply":"2023-07-21T06:42:59.30991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create frequency dictionary\nfreqs = build_freqs(tweets, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:42:59.312454Z","iopub.execute_input":"2023-07-21T06:42:59.312808Z","iopub.status.idle":"2023-07-21T06:43:03.808186Z","shell.execute_reply.started":"2023-07-21T06:42:59.312779Z","shell.execute_reply":"2023-07-21T06:43:03.807291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model prepare - sigmoid, gradient descent","metadata":{}},{"cell_type":"code","source":"# Define sigmoid function\ndef sigmoid(z): \n    h = h = 1/(1+np.exp(z*(-1)))    \n    return h","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:03.80944Z","iopub.execute_input":"2023-07-21T06:43:03.810663Z","iopub.status.idle":"2023-07-21T06:43:03.816182Z","shell.execute_reply.started":"2023-07-21T06:43:03.810592Z","shell.execute_reply":"2023-07-21T06:43:03.814576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradientDescent(x, y, theta, alpha, num_iters):\n\n    m = len(x)\n    for i in range(0, num_iters):\n        z = np.dot(x,theta)\n        h = sigmoid(z)\n        J = ((-1)/m)*(np.dot(np.transpose(y),np.log(h))+np.dot(np.transpose(1-y),np.log(1-h)))\n\n        # update the weights theta\n        theta = theta - ((alpha/m*(np.dot(np.transpose(x),(h-y)))))\n        \n    J = float(J)\n    return J, theta","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:03.817831Z","iopub.execute_input":"2023-07-21T06:43:03.818192Z","iopub.status.idle":"2023-07-21T06:43:03.829644Z","shell.execute_reply.started":"2023-07-21T06:43:03.818164Z","shell.execute_reply":"2023-07-21T06:43:03.828517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"def extract_features(tweet, freqs, process_tweet=process_tweet):\n    word_l = process_tweet(tweet)\n    x = np.zeros(3) \n    x[0] = 1 \n    \n    for word in word_l:\n\n        x[1] += freqs.get((word,1),0)\n\n        x[2] += freqs.get((word,0),0)\n    \n    x = x[None, :]\n    assert(x.shape == (1, 3))\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:03.831411Z","iopub.execute_input":"2023-07-21T06:43:03.832264Z","iopub.status.idle":"2023-07-21T06:43:03.842882Z","shell.execute_reply.started":"2023-07-21T06:43:03.832232Z","shell.execute_reply":"2023-07-21T06:43:03.841495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = tweets\ntrain_y = pd.DataFrame({'col':labels})\n\ntest_x = test['text']\ntest_y = test['target']","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:03.844162Z","iopub.execute_input":"2023-07-21T06:43:03.844482Z","iopub.status.idle":"2023-07-21T06:43:03.861431Z","shell.execute_reply.started":"2023-07-21T06:43:03.844456Z","shell.execute_reply":"2023-07-21T06:43:03.860439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Prediction Function","metadata":{}},{"cell_type":"code","source":"def predict_tweet(tweet, freqs, theta):\n\n    x = extract_features(tweet, freqs)\n    y_pred = sigmoid(np.dot(x,theta))\n\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:03.862819Z","iopub.execute_input":"2023-07-21T06:43:03.863813Z","iopub.status.idle":"2023-07-21T06:43:03.872781Z","shell.execute_reply.started":"2023-07-21T06:43:03.863778Z","shell.execute_reply":"2023-07-21T06:43:03.871547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performance monitor","metadata":{}},{"cell_type":"code","source":"def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n\n    y_hat = []\n    \n    for tweet in test_x:\n        y_pred = predict_tweet(tweet,freqs,theta)\n        \n        if y_pred > 0.5:\n            y_hat.append(1.0)\n        else:\n            y_hat.append(0.0)\n\n\n    #precision = np.dot(np.where(np.array(y_hat)==1,1,0),np.where(np.array(y_hat)==np.squeeze(test_y),1,0))/np.sum(np.array(y_hat)==1)\n    recall = np.dot(np.where(np.array(y_hat)==1,1,0),np.where(np.array(y_hat)==np.squeeze(test_y),1,0))/np.sum(np.array(test_y)==1)\n    accuracy = recall\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-07-21T07:19:09.814806Z","iopub.execute_input":"2023-07-21T07:19:09.815738Z","iopub.status.idle":"2023-07-21T07:19:09.824022Z","shell.execute_reply.started":"2023-07-21T07:19:09.815691Z","shell.execute_reply":"2023-07-21T07:19:09.823218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.dot(np.where((np.array([0,1])==np.array([0,1])),1,0), np.where(np.array([0,1])==np.array([0,1]),1,0))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T07:13:24.459941Z","iopub.execute_input":"2023-07-21T07:13:24.460337Z","iopub.status.idle":"2023-07-21T07:13:24.470473Z","shell.execute_reply.started":"2023-07-21T07:13:24.460308Z","shell.execute_reply":"2023-07-21T07:13:24.469225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.where(np.array([0,1])==np.array([0.1]),1,0)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T07:13:01.010126Z","iopub.execute_input":"2023-07-21T07:13:01.010601Z","iopub.status.idle":"2023-07-21T07:13:01.018632Z","shell.execute_reply.started":"2023-07-21T07:13:01.010566Z","shell.execute_reply":"2023-07-21T07:13:01.017664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"X = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\nY = train_y\n\n#for itr in [6000,8000,10000,12000]:\n#    for alpha in [1e-8,1e-9]:\n#        J, theta = gradientDescent(X, Y, np.zeros((3, 1)), alpha, itr)\n        # print(f\"The cost after training is {J:.8f}.\")\n        # print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n#        tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n#        print(f\"With iteration set as {itr}. alpha set as {alpha}. The logistic regression model's accuracy = {tmp_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-21T07:15:19.873098Z","iopub.execute_input":"2023-07-21T07:15:19.873818Z","iopub.status.idle":"2023-07-21T07:16:03.614166Z","shell.execute_reply.started":"2023-07-21T07:15:19.873773Z","shell.execute_reply":"2023-07-21T07:16:03.610792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-08,6000)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:08.471794Z","iopub.execute_input":"2023-07-21T06:43:08.472155Z","iopub.status.idle":"2023-07-21T06:43:31.289675Z","shell.execute_reply.started":"2023-07-21T06:43:08.472124Z","shell.execute_reply":"2023-07-21T06:43:31.28803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model, comment those as they are not needed\n\n# for tweet in ['there is a fire', 'there is sandstorm', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n#    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))    \n\n# my_tweet = 'alert, there is earthquake'\n# predict_tweet(my_tweet, freqs, theta)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:31.291396Z","iopub.execute_input":"2023-07-21T06:43:31.292175Z","iopub.status.idle":"2023-07-21T06:43:31.298997Z","shell.execute_reply.started":"2023-07-21T06:43:31.292132Z","shell.execute_reply":"2023-07-21T06:43:31.297568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Below code is to examine the prediction tweet by tweet, comment those as these are not needed\n\n# print('Label Predicted Tweet')\n# for x,y in zip(train_x,labels):\n#    y_hat = predict_tweet(x, freqs, theta)\n\n#    if np.abs(y - (y_hat > 0.5)) > 0:\n#        print('THE TWEET IS:', x)\n#        print('THE PROCESSED TWEET IS:', process_tweet(x))\n#        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:31.300752Z","iopub.execute_input":"2023-07-21T06:43:31.301348Z","iopub.status.idle":"2023-07-21T06:43:31.317705Z","shell.execute_reply.started":"2023-07-21T06:43:31.301306Z","shell.execute_reply":"2023-07-21T06:43:31.316016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use below code to test the model\n\nmy_tweet = 'Something is happening. I can see the flame from that building!'\nprint(process_tweet(my_tweet))\ny_hat = predict_tweet(my_tweet, freqs, theta)\nprint(y_hat)\nif y_hat > 0.5:\n    print('Potential disaster')\nelse: \n    print('Maybe not a disaster')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:31.319907Z","iopub.execute_input":"2023-07-21T06:43:31.320801Z","iopub.status.idle":"2023-07-21T06:43:31.336588Z","shell.execute_reply.started":"2023-07-21T06:43:31.320754Z","shell.execute_reply":"2023-07-21T06:43:31.333916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Prediction based on test data","metadata":{}},{"cell_type":"code","source":"y_hat_list = []\nfor x in validation['text']:\n    # print(x)\n    # x_process = process_tweet(x)\n    # print(x_process)\n    y_hat = predict_tweet(x, freqs, theta)\n    y_hat_list.append(int(y_hat > 0.5))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:31.339167Z","iopub.execute_input":"2023-07-21T06:43:31.340036Z","iopub.status.idle":"2023-07-21T06:43:34.236446Z","shell.execute_reply.started":"2023-07-21T06:43:31.339987Z","shell.execute_reply":"2023-07-21T06:43:34.235307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = pd.DataFrame({'target':y_hat_list})\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:34.237977Z","iopub.execute_input":"2023-07-21T06:43:34.238295Z","iopub.status.idle":"2023-07-21T06:43:34.262459Z","shell.execute_reply.started":"2023-07-21T06:43:34.238267Z","shell.execute_reply":"2023-07-21T06:43:34.261385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2023-07-21T06:43:42.745065Z","iopub.execute_input":"2023-07-21T06:43:42.745567Z","iopub.status.idle":"2023-07-21T06:43:42.759234Z","shell.execute_reply.started":"2023-07-21T06:43:42.74553Z","shell.execute_reply":"2023-07-21T06:43:42.758207Z"},"trusted":true},"execution_count":null,"outputs":[]}]}